{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0416 14:45:01.628971 140044708812608 __init__.py:56] Some hub symbols are not available because TensorFlow version is less than 1.14\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# prevent tensorflow from using GPU. Otherwise, run out of memory\n",
    "# https://stackoverflow.com/questions/44552585/prevent-tensorflow-from-accessing-the-gpu\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\"\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "\n",
    "import nltk\n",
    "import regex as re\n",
    "import emoji as em\n",
    "\n",
    "import itertools as it\n",
    "\n",
    "elmo = hub.Module(\"https://tfhub.dev/google/elmo/2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Pickles\n",
    "pickle_path = '/home/tim/Documents/Sentiment/Data/pickles'\n",
    "\n",
    "#x_train\n",
    "pickle_in = open(os.path.join(pickle_path,'elmo_embeddings.pickle'),\"rb\")\n",
    "elmo_dict = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elmo_dict length: 1193514\n"
     ]
    }
   ],
   "source": [
    "print(\"elmo_dict length:\", len(elmo_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key:\t 10000\n",
      "key:\t 20000\n",
      "key:\t 30000\n",
      "key:\t 40000\n",
      "key:\t 50000\n",
      "key:\t 60000\n",
      "key:\t 70000\n",
      "key:\t 80000\n",
      "key:\t 90000\n",
      "key:\t 100000\n",
      "key:\t 110000\n",
      "key:\t 120000\n",
      "key:\t 130000\n",
      "key:\t 140000\n",
      "key:\t 150000\n",
      "key:\t 160000\n",
      "key:\t 170000\n",
      "key:\t 180000\n",
      "key:\t 190000\n",
      "key:\t 200000\n",
      "key:\t 210000\n",
      "key:\t 220000\n",
      "key:\t 230000\n",
      "key:\t 240000\n",
      "key:\t 250000\n",
      "key:\t 260000\n",
      "key:\t 270000\n",
      "key:\t 280000\n",
      "key:\t 290000\n",
      "key:\t 300000\n",
      "key:\t 310000\n",
      "key:\t 320000\n",
      "key:\t 330000\n",
      "key:\t 340000\n",
      "key:\t 350000\n",
      "key:\t 360000\n",
      "key:\t 370000\n",
      "key:\t 380000\n",
      "key:\t 390000\n",
      "key:\t 400000\n",
      "key:\t 410000\n",
      "key:\t 420000\n",
      "key:\t 430000\n",
      "key:\t 440000\n",
      "key:\t 450000\n",
      "key:\t 460000\n",
      "key:\t 470000\n",
      "key:\t 480000\n",
      "key:\t 490000\n",
      "key:\t 500000\n",
      "key:\t 510000\n",
      "key:\t 520000\n",
      "key:\t 530000\n",
      "key:\t 540000\n",
      "key:\t 550000\n",
      "key:\t 560000\n",
      "key:\t 570000\n",
      "key:\t 580000\n",
      "key:\t 590000\n",
      "key:\t 600000\n",
      "key:\t 610000\n",
      "key:\t 620000\n",
      "key:\t 630000\n",
      "key:\t 640000\n",
      "key:\t 650000\n",
      "key:\t 660000\n",
      "key:\t 670000\n",
      "key:\t 680000\n",
      "key:\t 690000\n",
      "key:\t 700000\n",
      "key:\t 710000\n",
      "key:\t 720000\n",
      "key:\t 730000\n",
      "key:\t 740000\n",
      "key:\t 750000\n",
      "key:\t 760000\n",
      "key:\t 770000\n",
      "key:\t 780000\n",
      "key:\t 790000\n",
      "key:\t 800000\n",
      "key:\t 810000\n",
      "key:\t 820000\n",
      "key:\t 830000\n",
      "key:\t 840000\n",
      "key:\t 850000\n",
      "key:\t 860000\n",
      "key:\t 870000\n",
      "key:\t 880000\n",
      "key:\t 890000\n",
      "key:\t 900000\n",
      "key:\t 910000\n",
      "key:\t 920000\n",
      "key:\t 930000\n",
      "key:\t 940000\n",
      "key:\t 950000\n",
      "key:\t 960000\n",
      "key:\t 970000\n",
      "key:\t 980000\n",
      "key:\t 990000\n",
      "key:\t 1000000\n",
      "key:\t 1010000\n",
      "key:\t 1020000\n",
      "key:\t 1030000\n",
      "key:\t 1040000\n",
      "key:\t 1050000\n",
      "key:\t 1060000\n",
      "key:\t 1070000\n",
      "key:\t 1080000\n",
      "key:\t 1090000\n",
      "key:\t 1100000\n",
      "key:\t 1110000\n",
      "key:\t 1120000\n",
      "key:\t 1130000\n",
      "key:\t 1140000\n",
      "key:\t 1150000\n",
      "key:\t 1160000\n",
      "key:\t 1170000\n",
      "key:\t 1180000\n",
      "key:\t 1190000\n"
     ]
    }
   ],
   "source": [
    "with open('elmo.txt', 'w') as f:\n",
    "    i = 0\n",
    "    for key, value in elmo_dict.items():\n",
    "        \n",
    "        \n",
    "        f.write('%s\\t' % (key))\n",
    "        for x in value:\n",
    "            f.write('%s ' % (x))\n",
    "        i += 1\n",
    "        if i %10000 == 0:\n",
    "            print(\"key:\\t\", i)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = elmo_dict['<user>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in t:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in elmo_dict.items():\n",
    "    print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "with open('file.txt', 'w') as file:\n",
    "     file.write(json.dumps(elmo_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTIONS\n",
    "\n",
    "# process tweet\n",
    "def preprocess(tweet):\n",
    "    #define hashtag pattern\n",
    "    hashtag_define = re.compile ('#')\n",
    "    #define mention pattern \n",
    "    mentions_define = re.compile('@[^\\s]+')\n",
    "    #define link pattern\n",
    "    link_define = re.compile('https?://[^\\s]+')\n",
    "    # Haystack define\n",
    "    haystack_define = re.compile('(RT)')\n",
    "    # define long spaces\n",
    "    extra_spaces_define = re.compile('\\s{2,}')\n",
    "    #remove hashtags\n",
    "    tweet_refine = hashtag_define.sub('',tweet)\n",
    "    # remove mentions\n",
    "    tweet_refine = mentions_define.sub('',tweet_refine)\n",
    "    # remove links\n",
    "    tweet_refine = link_define.sub('',tweet_refine)\n",
    "    # remove haystack\n",
    "    tweet_refine = haystack_define.sub('',tweet_refine)\n",
    "    #replace long spaces with one space\n",
    "    tweet_refine = extra_spaces_define.sub(' ',tweet_refine)\n",
    "    # convert emoticons into words\n",
    "    tweet_refine = em.demojize(tweet_refine)\n",
    "    return tweet_refine\n",
    "\n",
    "# Create tokens with simple split on whitespace\n",
    "def simple_token(s):\n",
    "    return s.split()\n",
    "\n",
    "# Get token length\n",
    "def token_length(s):\n",
    "    return len(s)\n",
    "\n",
    "# Create \"embedding lists\" of equal size -- pad with empty characters, e.g. \"\"\n",
    "# https://stackoverflow.com/questions/24066904/most-pythonic-way-to-extend-a-list-to-exactly-a-certain-length\n",
    "def pad_list(some_list, target_len):\n",
    "    return some_list[:target_len] + [\"\"]*(target_len - len(some_list))\n",
    "\n",
    "# Get the elmo embeddings\n",
    "def elmo_tweet_embedder(tokens,len_list):\n",
    "    \n",
    "    tokens_input = tokens #load a tweet\n",
    "    tokens_length = len_list # get length of tweet\n",
    "\n",
    "    #create embedding\n",
    "    embedding_tensor = elmo(inputs={\"tokens\":tokens_input,\"sequence_len\":tokens_length},\n",
    "                            signature=\"tokens\", as_dict=True)[\"word_emb\"] # <-- passing in a list instead of [word]\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        embedding = sess.run(embedding_tensor)\n",
    "        return embedding\n",
    "\n",
    "# combine all together\n",
    "def make_embeddings(df, max_seq_len):\n",
    "    m = max_seq_len\n",
    "    \n",
    "    # create a token column in df\n",
    "    df['tokens'] = df['text'].apply(simple_token)\n",
    "    print(\"Token creation complete\")\n",
    "    \n",
    "    # pad the tokens\n",
    "    df['tokens'] = df.apply(lambda x: pad_list(x['tokens'],m),axis=1)\n",
    "    print(\"Token padding complete\")\n",
    "    \n",
    "    # split data into smaller batches of size 100. http://bit.ly/2P4J8HJ\n",
    "    text_batches = [df['tokens'][i:i+5000] for i in range(0,df.shape[0],5000)]\n",
    "    len_lists = [[m] * len(x) for x in text_batches]\n",
    "\n",
    "    # create list of sentiments (y values)\n",
    "    y = df['sent'].to_numpy()\n",
    "    \n",
    "    # make embeddings\n",
    "    embeddings = []\n",
    "    for i in range(0,len(text_batches)):\n",
    "        print(\"batch\",i)\n",
    "        elmo_train = elmo_tweet_embedder(text_batches[i].tolist(),len_lists[i])\n",
    "        embeddings.append(elmo_train)\n",
    "        \n",
    "    embeddings = np.concatenate(embeddings, axis=0)\n",
    "    \n",
    "    return embeddings, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#ORIGINAL\n",
    "\n",
    "path = '/home/tim/Documents/Sentiment/Data/dictionary_raw'\n",
    "file = 'vocab_complete.txt'\n",
    "\n",
    "# path = '/home/tim/Documents/Sentiment/Data/dictionary_raw'\n",
    "# file = 'text.txt'\n",
    "\n",
    "p = os.path.join(path,file)\n",
    "\n",
    "# open file\n",
    "f = open(p, \"r\")\n",
    "lines = f.readlines()\n",
    "\n",
    "#remove new line symbol\n",
    "l=[]\n",
    "for x in lines:\n",
    "    x = x.rstrip(\"\\n\")\n",
    "    l.append(x)\n",
    "    \n",
    "# create df\n",
    "col_names = ['sent']\n",
    "df = pd.DataFrame(l,columns=col_names)\n",
    "df['text']=df['sent']\n",
    "f.close()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df['sent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_embeddings(df, 1)\n",
    "d = dict()\n",
    "for i in range(len(y)):\n",
    "    d[y[i]] = X[i][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Pickles\n",
    "pickle_path = '/home/tim/Documents/Sentiment/Data/pickles'\n",
    "\n",
    "# elmo embeddings\n",
    "pickle_out = open(os.path.join(pickle_path,'elmo_embeddings.pickle'),\"wb\")\n",
    "pickle.dump(d,pickle_out)\n",
    "pickle_out.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
