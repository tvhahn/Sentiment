{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# prevent tensorflow from using GPU. Otherwise, run out of memory\n",
    "# https://stackoverflow.com/questions/44552585/prevent-tensorflow-from-accessing-the-gpu\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\"\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "\n",
    "elmo = hub.Module(\"https://tfhub.dev/google/elmo/2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTIONS\n",
    "\n",
    "# Create tokens with simple split on whitespace\n",
    "def simple_token(s):\n",
    "    return s.split()\n",
    "\n",
    "# Get token length\n",
    "def token_length(s):\n",
    "    return len(s)\n",
    "\n",
    "# Create \"embedding lists\" of equal size -- pad with empty characters, e.g. \"\"\n",
    "# https://stackoverflow.com/questions/24066904/most-pythonic-way-to-extend-a-list-to-exactly-a-certain-length\n",
    "def pad_list(some_list, target_len):\n",
    "    return some_list[:target_len] + [\"\"]*(target_len - len(some_list))\n",
    "\n",
    "# Get the elmo embeddings\n",
    "def elmo_tweet_embedder(tokens,len_list):\n",
    "    \n",
    "    tokens_input = tokens #load a tweet\n",
    "    tokens_length = len_list # get length of tweet\n",
    "\n",
    "    #create embedding\n",
    "    embedding_tensor = elmo(inputs={\"tokens\":tokens_input,\"sequence_len\":tokens_length},\n",
    "                            signature=\"tokens\", as_dict=True)[\"word_emb\"] # <-- passing in a list instead of [word]\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        embedding = sess.run(embedding_tensor)\n",
    "        return embedding\n",
    "\n",
    "# combine all together\n",
    "def make_embeddings(df, max_seq_len):\n",
    "    m = max_seq_len\n",
    "    \n",
    "    # create a token column in df\n",
    "    df['tokens'] = df['text'].apply(simple_token)\n",
    "    \n",
    "    # pad the tokens\n",
    "    df['tokens'] = df.apply(lambda x: pad_list(x['tokens'],m),axis=1)\n",
    "    \n",
    "    # split data into smaller batches of size 100. http://bit.ly/2P4J8HJ\n",
    "    text_batches = [df['tokens'][i:i+100] for i in range(0,df.shape[0],100)]\n",
    "    len_lists = [[m] * len(x) for x in text_batches]\n",
    "\n",
    "    # create list of sentiments (y values)\n",
    "    y = df['sent'].to_numpy()\n",
    "    \n",
    "    # make embeddings\n",
    "    embeddings = []\n",
    "    for i in range(0,len(text_batches)):\n",
    "        elmo_train = elmo_tweet_embedder(text_batches[i].tolist(),len_lists[i])\n",
    "        embeddings.append(elmo_train)\n",
    "        \n",
    "    embeddings = np.concatenate(embeddings, axis=0)\n",
    "    \n",
    "    return embeddings, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train start\n",
      "x_dev start\n",
      "x_test start\n"
     ]
    }
   ],
   "source": [
    "path = '/home/tim/Documents/Sentiment/Data/processed/'\n",
    "file_list = os.listdir(path)\n",
    "\n",
    "# create the data set names\n",
    "names = []\n",
    "for f in file_list:\n",
    "    f = f.split('.')\n",
    "    l = [\"x_\"+f[0],\"y_\"+f[0]]\n",
    "    names.append(l)\n",
    "\n",
    "data_names = names\n",
    "    \n",
    "# index through files and create x and y sets\n",
    "for i in range(0,len(file_list)):\n",
    "    \n",
    "    s = names\n",
    "   \n",
    "    p = os.path.join(path,file_list[i])\n",
    "    col_names = ['text','sent']\n",
    "    df = pd.read_csv(p,delimiter=\"\\t\",names=col_names)\n",
    "\n",
    "    print(s[i][0], \"start\")\n",
    "    s[i][0], s[i][1] = make_embeddings(df, 80) \n",
    "\n",
    "x_train = s[0][0]\n",
    "y_train = s[0][1]\n",
    "x_dev = s[1][0]\n",
    "y_dev = s[1][1]\n",
    "x_test = s[2][0]\n",
    "y_test = s[2][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Pickles\n",
    "pickle_path = '/home/tim/Documents/Sentiment/Data/pickles'\n",
    "\n",
    "# x_train\n",
    "pickle_out = open(os.path.join(pickle_path,'x_train.pickle'),\"wb\")\n",
    "pickle.dump(x_train, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "# y_train\n",
    "pickle_out = open(os.path.join(pickle_path,'y_train.pickle'),\"wb\")\n",
    "pickle.dump(y_train, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "# x_dev\n",
    "pickle_out = open(os.path.join(pickle_path,'x_dev.pickle'),\"wb\")\n",
    "pickle.dump(x_dev, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "# y_dev\n",
    "pickle_out = open(os.path.join(pickle_path,'y_dev.pickle'),\"wb\")\n",
    "pickle.dump(y_dev, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "# x_test\n",
    "pickle_out = open(os.path.join(pickle_path,'x_test.pickle'),\"wb\")\n",
    "pickle.dump(x_test, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "# y_test\n",
    "pickle_out = open(os.path.join(pickle_path,'y_test.pickle'),\"wb\")\n",
    "pickle.dump(y_test, pickle_out)\n",
    "pickle_out.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
